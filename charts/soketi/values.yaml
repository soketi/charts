# Default values for soketi.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

strategy: {}
  # type: RollingUpdate
  # rollingUpdate:
  #   maxSurge: 0
  #   maxUnavailable: 1

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# soketi is the server that will act like the Pusher server.
app:
  image:
    repository: quay.io/soketi/soketi
    pullPolicy: IfNotPresent
    tag: "1.3.0-16-alpine"
    # You can use distroless to avoid Remote Code Execution attacks in-cluster.
    # https://github.com/soketi/soketi/pull/178
    # tag: "1.3-16-distroless"

  # It's recommended to run the server with
  # maximum memory that would match the resource limits
  # for maximum memory space efficiency.
  command:
    - node
    - --max-old-space-size=256
    - --max_old_space_size=256
    - --optimize_for_size
    - --optimize-for-size
    - /app/bin/server.js
    - start

  # Add extra env to the soketi container.
  extraEnv: []
  # - name: POD_ID
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: metadata.name
  # - name: NODE_ID
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: spec.nodeName

  # Extra volumes to mount on the container.
  extraVolumeMounts: []
  # - name: some-folder
  #   mountPath: /some/path

  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  resources:
    limits:
      cpu: 250m
      memory: 256Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # The mode the app is running with. (full, worker, server)
  # Read the documentation: https://docs.soketi.app/advanced-usage/horizontal-scaling/running-modes
  mode: full

  # Multicast will enable settings for pods to support multicast.
  multicast:
    enabled: false
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet

# The Network Watcher is watching for container maximum allocated
# resources and once a specific threshold is exceeded, it will tell the
# soketi server to stop accepting new connections by marking the Pod
# as not ready. The existing connections will still be persisted.
networkWatcher:
  enabled: false

  image:
    repository: quay.io/soketi/network-watcher
    pullPolicy: IfNotPresent
    tag: "6.3"

  # Extra volumes to mount on the container.
  extraVolumeMounts: []
  # - name: some-folder
  #   mountPath: /some/path

  command:
    - php
    - /app/network-watcher
    - network:watch

  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

  # The above threshold (in percent) after the pod becomes
  # unavailable to serve new connections. 85% is a good threshold,
  # because the still-opened connections will add some new members in
  # the presence channels if they join or leave new ones.
  threshold: 85

  # The interval in seconds between checks. This will check the Prometheus
  # metrics and will decide wether the threshold was reached and will prevent
  # new connections from joining this pod.
  interval: 1

# BullMQ Exporter is a tool to export the BullMQ metrics to your Prometheus scraper.
# This way, you can scale queue workers independently.
# Read docs: https://github.com/UpHabit/bull_exporter
bullExporter:
  enabled: false

  replicaCount: 1

  strategy: {}
  # type: RollingUpdate
  # rollingUpdate:
  #   maxSurge: 0
  #   maxUnavailable: 1

  image:
    repository: uphabit/bull_exporter
    pullPolicy: IfNotPresent
    tag: latest

  imagePullSecrets: []

  # Extra volumes to mount on the container.
  extraVolumeMounts: []
  # - name: some-folder
  #   mountPath: /some/path

  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

  pdb:
    enabled: false
    minAvailable: 1
    # maxUnavailable: 25%

  # Extra volumes to attach to the deployment.
  extraVolumes: []
  # - name: some-folder
  #   emptyDir: {}

  redisHost: redis://redis-headless:6379/0

  # The list of queues.
  queues:
    - client_event_webhooks
    - member_added_webhooks
    - member_removed_webhooks
    - channel_vacated_webhooks
    - channel_occupied_webhooks

  # The prefix for queues.
  prefix: bull

  # The stat prefix for the Prometheus names.
  statPrefix: bull_queue_

  service:
    annotations: {}
    #  prometheus.io/scrape: 'true'
    #  prometheus.io/port: '9538'

  serviceMonitor:
    enabled: false
    scrapeInterval: 5s
    scrapeTimeout: 3s

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext:
    runAsGroup: 65534
    runAsUser: 65534
    runAsNonRoot: true
    privileged: false
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - all

  nodeSelector: {}

  tolerations: []

  affinity: {}

serviceMonitor:
  enabled: false
  scrapeInterval: 5s
  scrapeTimeout: 3s

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 6001

  annotations: {}
  # Set annotations for the service.

ingress:
  enabled: false
  # class: "nginx"
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: soketi-example.local
      paths: []

    # - host: soketi.test
    #   paths:
    #     - /
  tls: []
  #  - secretName: soketi-example-tls
  #    hosts:
  #      - soketi-example.local

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

  # Set the behavior for the autoscaler.
  # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-configurable-scaling-behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 1800
    scaleUp:
      stabilizationWindowSeconds: 10

  # Custom Metrics will be appended to the default CPU/Memory resources (if they're enabled).
  customMetrics: []
  # - type: Pods
  #   pods:
  #     metric:
  #       name: nginx_process_utilization
  #     target:
  #       type: AverageValue
  #       averageValue: "50"

pdb:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 25%

livenessProbe:
  httpGet:
    path: /
    port: 6001
    httpHeaders:
      - name: X-Kube-Healthcheck
        value: "Yes"
  initialDelaySeconds: 5
  periodSeconds: 2
  failureThreshold: 3
  successThreshold: 1
  # terminationGracePeriodSeconds: 30

readinessProbe:
  httpGet:
    path: /accept-traffic
    port: 6001
    httpHeaders:
      - name: X-Kube-Healthcheck
        value: "Yes"
  initialDelaySeconds: 5
  periodSeconds: 1
  failureThreshold: 1
  successThreshold: 1

# In some cases, closing a Pod that has a lot of connections
# will disconnect them, and stats would need some time to catch up with the disconnections,
# so a bigger termination grace period would be suitable.
terminationGracePeriodSeconds: 30

nodeSelector: {}

tolerations: []

affinity: {}

# Extra volumes to attach to the deployment.
extraVolumes: []
# - name: some-folder
#   emptyDir: {}

# Extra containers to run in the deployment.
extraContainers: []

# Extra init containers to run in the deployment.
extraInitContainers: []
