# Default values for pws.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# pWS is the server that will act like the Pusher server.
app:
  image:
    repository: quay.io/soketi/pws
    pullPolicy: IfNotPresent
    tag: "0.8-16-alpine"

  # It's recommended to run the server with
  # maximum memory that would match the resource limits
  # for maximum memory space efficiency.
  command:
    - node
    - --max-old-space-size=256
    - --max_old_space_size=256
    - --optimize_for_size
    - --optimize-for-size
    - /app/bin/server.js
    - start

  # Add extra env to the pWS container.
  extraEnv: []
  # - name: POD_ID
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: metadata.name
  # - name: NODE_ID
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: spec.nodeName

  # Extra volumes to mount on the container.
  extraVolumeMounts: []
  # - name: some-folder
  #   mountPath: /some/path

  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  resources:
    limits:
      cpu: 250m
      memory: 256Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# The Network Watcher is watching for container maximum allocated
# resources and once a specific threshold is exceeded, it will tell the
# pWS server to stop accepting new connections by marking the Pod
# as not ready. The existing connections will still be persisted.
networkWatcher:
  enabled: false

  image:
    repository: quay.io/soketi/network-watcher
    pullPolicy: IfNotPresent
    tag: "4.6"

  # Extra volumes to mount on the container.
  extraVolumeMounts: []
  # - name: some-folder
  #   mountPath: /some/path

  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

  # The above threshold (in percent) after the pod becomes
  # unavailable to serve new connections. 85% is a good threshold,
  # because the still-opened connections will add some new members in
  # the presence channels if they join or leave new ones.
  threshold: 85

  # The interval in seconds between checks. This will check the Prometheus
  # metrics and will decide wether the threshold was reached and will prevent
  # new connections from joining this pod.
  interval: 1

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 6001

  annotations: {}
  # Set annotations for the service.

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: pws-example.local
      paths: []

    # - host: pws.test
    #   paths:
    #     - /
  tls: []
  #  - secretName: pws-example-tls
  #    hosts:
  #      - pws-example.local

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

  # Set the behavior for the autoscaler.
  # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-configurable-scaling-behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 1800
    scaleUp:
      stabilizationWindowSeconds: 10

  # Custom Metrics will be appended to the default CPU/Memory resources (if they're enabled).
  customMetrics: []
  # - type: Pods
  #   pods:
  #     metric:
  #       name: nginx_process_utilization
  #     target:
  #       type: AverageValue
  #       averageValue: "50"

pdb:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 25%

# In some cases, closing a Pod that has a lot of connections
# will disconnect them, and stats would need some time to catch up with the disconnections,
# so a bigger termination grace period would be suitable.
terminationGracePeriodSeconds: 30

nodeSelector: {}

tolerations: []

affinity: {}

# Extra volumes to attach to the deployment.
extraVolumes: []
# - name: some-folder
#   emptyDir: {}

# Extra containers to run in the deployment.
extraContainers: []

# Extra init containers to run in the deployment.
extraInitContainers: []

serviceMonitor:
  ## If true, a ServiceMonitor CRD is created for a prometheus operator
  ## https://github.com/coreos/prometheus-operator
  ##
  enabled: false
  #  namespace: monitoring
  labels: {}
  interval: 30s
  scrapeTimeout: 10s
  scrapePath: /metrics
  scheme: http
  relabelings: []
  targetLabels: []
  metricRelabelings: []
  sampleLimit: 0

prometheusRule:
  ## If true, a PrometheusRule CRD is created for a prometheus operator
  ## https://github.com/coreos/prometheus-operator
  ##
  ## The rules will be processed as Helm template, allowing to set variables in them.
  enabled: false
  #  namespace: monitoring
  labels: {}
  rules: []
